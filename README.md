This repository contains the complete academic and hands-on materials used in the Faculty Development Program (FDP) on:

Generative AI, Large Language Models (LLMs), and Transformers

The FDP was organized as part of an IEEE-supported academic program, with a strong focus on conceptual clarity, intuition, and practical exposure to modern Transformer-based AI systems.

üè´ FDP Details

Program Title:
Generative AI, Large Language Models (LLMs), and Transformers

Organized by:
Department of Information Technology,
Sri Sai Ram Institute of Technology, Chennai

Session Flow (Pedagogical Design)

The FDP content follows a carefully designed learning progression:

**Context & Motivation**

AI vs ML vs DL vs Transformers vs GenAI

Why RNNs and LSTMs fail at scale

**Core Breakthrough**

Attention Is All You Need

Recurrence vs Attention

Spotlight analogy for self-attention

**Transformer Architecture**

Self-Attention

Multi-Head Attention

Positional Encoding

Decoder-only LLMs (GPT-style)

**Types of Transformers**

Encoder-only, Decoder-only, Encoder‚ÄìDecoder

Vision, Multimodal, Efficient, Domain-specific models

**Hands-On Demos (No Training Required)**

Attention from scratch

Semantic similarity

Masked language modeling

Prompt sensitivity

Text classification

**Applications, Limitations & Research Directions** 

NLP, Vision, Biomedical signals

Hallucinations, safety, ethics

Open research problems

üõ†Ô∏è Requirements

To run the notebooks locally or in Google Colab:

pip install transformers sentence-transformers torch numpy scikit-learn
